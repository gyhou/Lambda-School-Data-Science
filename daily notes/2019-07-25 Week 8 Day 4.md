
2019-07-25 Week 8 Day 4    
Unit 2 Sprint 3 Module 4   

Meeting ID  
https://lambdaschool.zoom.us/j/246120420

Ryan Herr:lambda-red: 2:25 PM  
https://youtu.be/dAtQUNZEWVI  
YouTubeYouTube | Lambda School  
Model Interpretation for DS5 w/Ryan Herr  

Ryan Herr:lambda-red: 10:55 AM  
@channel Today’s notebooks (yes, plural!) are in the Github repo:   
https://github.com/LambdaSchool/DS-Unit-2-Applied-Modeling/tree/master/module4-model-interpretation   

You can follow along live on Google Colab:  
- https://colab.research.google.com/drive/16ovHpiI_CRFj7YkmG_BbKbcILV-VSilb  
- https://colab.research.google.com/drive/1w3WpKJwrSyP9JMtyQ9W0GIpywjcM7cAG  

We’re using 3 new libraries today!  
- eli5 (https://github.com/TeamHG-Memex/eli5)  
- PDPbox (https://github.com/SauceCat/PDPbox)  
- shap (https://github.com/slundberg/shap)  

So, if you’re working locally, you might want to start installing:  
- conda install -c conda-forge eli5 shap  
- pip install pdpbox  

For your portfolio project next week, remember to review this info about dates, requirements, examples, & datasets.  
https://gist.github.com/rrherr/e1e6c68b2cee9408ae04bac9aa368808  
By Friday, July 26, 5pm Pacific, submit this form with your dataset’s URL:   
https://forms.gle/X1GbH475D5yYKTdr9  

Ryan Herr:lambda-red: 10:08 AM  
Today we’ll learn about partial dependence plots and other visualizations for model interpretation!   https://twitter.com/ChristophMolnar/status/1066398522608635904  
Christoph MolnarChristoph Molnar @ChristophMolnar  
Partial dependence plots show how a feature affects predictions of a #MachineLearning model on average.  
Learn how PD plots are computed with this cool animation. :sunglasses:   

Ryan Herr:lambda-red: 2:34 PM  
@here To recap today’s assignment:   
Use the Caterpillar dataset (or any dataset of your choice). Make these types of visualizations for model interpretation:
- Permutation Importances, compared to default Feature Importances
- Partial Dependence Plot
- Shapley Values

Share at least 1 of your visualizations on Slack. Commit your notebook to your fork of the GitHub repo.  
For your portfolio project next week, remember to review this info about dates, requirements, examples, & datasets.  
https://gist.github.com/rrherr/e1e6c68b2cee9408ae04bac9aa368808   
By Friday, July 26, 5pm Pacific, submit this form with your dataset’s URL:   
https://forms.gle/X1GbH475D5yYKTdr9  
Stretch Goals: See the supplementary links in the notebooks!   

Ryan Herr:lambda-red: 2:32 PM  
replied to a thread:  
would have to write a class w/ fit and transform methods to shove into pipeline, no?  
Here’s some code from a DS4 student for a related goal, using the eval_set parameter for xgboost early stopping, within a complete scikit-learn pipeline:  
```
class XGRegressorEval(XGBRegressor):
  def fit(self, *args, **kwargs):
    return super().fit(*args, eval_set=eval_set, eval_metric='rmse', 
          early_stopping_rounds=10, **kwargs)
    
pipeline = make_pipeline(ce.OrdinalEncoder(), XGRegressorEval(n_estimators=1000, n_jobs=-1))
```


