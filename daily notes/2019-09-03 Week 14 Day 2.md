2019-09-03 Week 14 Day 2  
Unit 4 Sprint 1 Module 1&2  
Natural Language Processing - Introduction  
Vector Representations   

traning kit  
https://learn.lambdaschool.com/ds/module/rec5r3BWhwU0xnsUg/  
https://learn.lambdaschool.com/ds/module/rec0O4tJizjI1C6EA/  

================================================

Jon-Cody Sokoll [12:52]   
Unit 4 Sprint 1 Module 2 Project
Last edited < 1 minute ago
For this project, I would like to analyze job data science job postings from indeed.com. Apply vector representations and similarity queries to help you identify jobs that you want to apply for. You can find the notebook for the projec here:   
https://github.com/LambdaSchool/DS-Unit-4-Sprint-1-NLP/blob/master/module2-vector-representations/LS_DS_412_Vector_Representations_Assignment.ipynb    

There is a csv file of job listings in the module data folder to help you get started, but I highly incourage you to scrape indeed for more recent job listings. Have fun! :slightly_smiling_face: :python:

download the large model  
`python -m spacy download en_core_web_lg` 
  
```
import spacy
nlp = spacy.load('en_core_web_lg')

from collections import Counter
def count():
    pass
wc = count(shops['tokens']) # word counts

import squarify
import matplotlib.pyplot as plt
wc_top20
wc_bad = count(shops[shops['rating']<4]['tokens'])

from sklearn.metrics.pairwise import cosine_similarity
```

================================================

Han Lee [03:58]  
FYI, the following pypi packages for spacy were removed but still lives on `requirements.txt`

```
en-core-web-lg==2.0.0
en-core-web-md==2.0.0
en-core-web-sm==2.0.0
```

The proper download method is

```python -m spacy download en_core_web_sm```

Han Lee [05:29]  
For anyone who wants to use pipenv for Unit-4-Sprint-1, without any unused packages in `requirements.txt` or `environment_mac/windows.yml`

Pipfile  
```
[[source]]
name = "pypi"
url = "https://pypi.org/simple"
verify_ssl = true

[dev-packages]

[packages]
spacy = "*"
pandas = "*"
matplotlib = "*"
nltk = "*"
gensim = "*"
pyldavis = "*"
scikit-learn = "*"
seaborn = "*"
jupyter = "*"

[requires]
python_version = "3.7"
```

Quinn [16:33]  
@channel *Conda Environments*   
You will be completing each module this sprint on your machine. We will be using conda environments to manage the packages and their dependencies for this sprint's content. In a classroom setting, instructors typically abstract away environment for you. However, environment management is an important professional data science skill. We showed you how to manage environments using pipvirtual env during Unit 3, but in this sprint, we will introduce an environment management tool common in the data science community:

`conda`: Package, dependency and environment management for any languageâ€”Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.

The easiest way to install `conda` on your machine is via the Anaconda Distribution of Python & R. Once you have `conda` installed, read "_A Guide to Conda Environments_". This article will provide an introduce into some of the `conda` basics. If you need some additional help getting started, the official "Setting started with `conda`" guide will point you in the right direction.

To get the sprint environment setup:

Open your command line tool (Terminal for MacOS, Anaconda Prompt for Windows)   
Navigate to the folder with this sprint's content. There should be both `environment_mac.yml` and a `environment_windows.yml`.
Run `conda env create -n U4-S1-NLP -f /path/to/environment_mac.yml` => You should replace the operating system if you're a windows user. You can also rename the environment if you would like. Once the command completes, your conda environment should be ready.
We are going to also add an Ipython Kernel reference to your `conda` environment, so we can use it from JupyterLab. You will need to 'activate' the `conda` environment: `source activate U4-S1-NLP` on Terminal or `conda activate U4-S1-NLP` on Anaconda Prompt.
Next run `python -m ipykernel install --user --name U4-S1-NLP --display-name "U4-S1-NLP (Python3)"` => This will add a json object to an ipython file, so JupterLab will know that it can use this isolated instance of Python. :slightly_smiling_face:
Deactivate your conda environment and launch JupyterLab. You should know see "U4-S1-NLP (Python3)" in the list of available kernels on launch screen.

spaCy in Google Colab   
https://colab.research.google.com/drive/1MXN2CtRX8N5msp7IbZ0Xh5gjQYyFv8AU
